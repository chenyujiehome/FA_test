{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d51b3c74-c9aa-4d3c-ad0d-674eb4f46f33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/FA/bin/python\n",
      "['/root/autodl-tmp/FA_test_A100', '/root/anaconda3/envs/FA/lib/python38.zip', '/root/anaconda3/envs/FA/lib/python3.8', '/root/anaconda3/envs/FA/lib/python3.8/lib-dynload', '', '/root/anaconda3/envs/FA/lib/python3.8/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf18e2d2-4b47-45f8-a82e-06d5c5ae8289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from flash_attn.utils.benchmark import benchmark_all, benchmark_forward, benchmark_backward, benchmark_combined\n",
    "from flash_attn.bert_padding import unpad_input, pad_input\n",
    "from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func\n",
    "\n",
    "\n",
    "def attention_ref(qkv, attn_mask, dropout_p, upcast=False, causal=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        qkv: (batch_size, seqlen, 3, nheads, head_dim)\n",
    "        attn_mask: (batch_size, seqlen)\n",
    "        dropout_p: float\n",
    "    Output:\n",
    "        output: (batch_size, seqlen, nheads, head_dim)\n",
    "        attention: softmax after dropout\n",
    "    \"\"\"\n",
    "    q, k, v = (qkv.float() if upcast else qkv).unbind(dim=2)\n",
    "    seqlen = qkv.shape[1]\n",
    "    d = qkv.shape[-1]\n",
    "    scores = torch.einsum('bthd,bshd->bhts', q, k / math.sqrt(d))\n",
    "    scores.masked_fill_(rearrange(~attn_mask, 'b s -> b 1 1 s'), float('-inf'))\n",
    "    if causal:\n",
    "        causal_mask = torch.triu(torch.ones(seqlen, seqlen, dtype=torch.bool, device=qkv.device), 1)\n",
    "        scores.masked_fill_(causal_mask, float('-inf'))\n",
    "    attention = torch.softmax(scores, dim=-1)\n",
    "    attention_drop = F.dropout(attention, dropout_p)\n",
    "    output = torch.einsum('bhts,bshd->bthd', attention_drop , v)\n",
    "    # return output.to(dtype=qkv.dtype), attention.to(dtype=qkv.dtype)\n",
    "    return output.to(dtype=qkv.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74a054-4b0c-436d-a520-4bde4ecb2db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "repeats = 30\n",
    "batch_size = 64\n",
    "nheads = 16\n",
    "seqlen = 1024\n",
    "n = 1024\n",
    "d = n // nheads\n",
    "dropout_p = 0.1\n",
    "causal = False\n",
    "dtype = torch.float16\n",
    "device = 'cuda'\n",
    "\n",
    "x = torch.randn(batch_size, seqlen, n, device='cuda', dtype=dtype, requires_grad=True)\n",
    "Wqkv = torch.nn.Linear(nheads * d, 3 * nheads * d, device=device, dtype=dtype)\n",
    "\n",
    "lengths = torch.randint(seqlen - 20, seqlen, (batch_size, 1), device='cuda')\n",
    "attention_mask_bool = repeat(torch.arange(seqlen, device='cuda'), 's -> b s', b=batch_size) < lengths\n",
    "attention_mask = torch.zeros(batch_size, seqlen, device='cuda', dtype=dtype)\n",
    "attention_mask[~attention_mask_bool] = -10000.0\n",
    "attention_mask = rearrange(attention_mask, 'b s -> b 1 1 s')\n",
    "\n",
    "x_unpad, indices, cu_seqlens, max_seqlen_in_batch = unpad_input(x, attention_mask_bool)\n",
    "qkv_unpad = rearrange(Wqkv(x_unpad), 'nnz (t h d) -> nnz t h d', t=3,\n",
    "                      h=nheads).detach().requires_grad_()\n",
    "qkv = rearrange(Wqkv(x), 'b s (t h d) -> b s t h d', t=3, h=nheads).detach().requires_grad_()\n",
    "\n",
    "fn = lambda qkv_unpad: flash_attn_unpadded_qkvpacked_func(\n",
    "    qkv_unpad, cu_seqlens, max_seqlen_in_batch, dropout_p, causal=causal\n",
    ")\n",
    "benchmark_all(fn, qkv_unpad, repeats=repeats, desc='FlashAttention')\n",
    "fn = lambda qkv: attention_ref(qkv, attention_mask_bool, dropout_p, causal=causal)\n",
    "benchmark_all(fn, qkv, repeats=repeats, desc='PyTorch Standard Attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9753f12b-682b-4714-b5bd-3913d4249a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf616b97-fc58-4781-92fb-fe0fbcd492f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
