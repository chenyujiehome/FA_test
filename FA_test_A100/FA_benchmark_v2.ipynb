{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the newest triton version with\n",
    "# pip install \"git+https://github.com/openai/triton.git#egg=triton&subdirectory=python\"\n",
    "import pickle\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from flash_attn.utils.benchmark import benchmark_all, benchmark_forward, benchmark_backward\n",
    "from flash_attn.utils.benchmark import benchmark_fwd_bwd, benchmark_combined\n",
    "\n",
    "from flash_attn import flash_attn_qkvpacked_func\n",
    "\n",
    "try:\n",
    "    from triton.ops.flash_attention import attention as attention_triton\n",
    "except ImportError:\n",
    "    attention_triton = None\n",
    "\n",
    "try:\n",
    "    import xformers.ops as xops\n",
    "except ImportError:\n",
    "    xops = None\n",
    "\n",
    "\n",
    "def flops(batch, seqlen, headdim, nheads, causal, mode=\"fwd\"):\n",
    "    assert mode in [\"fwd\", \"bwd\", \"fwd_bwd\"]\n",
    "    f = 4 * batch * seqlen**2 * nheads * headdim // (2 if causal else 1)\n",
    "    return f if mode == \"fwd\" else (2.5 * f if mode == \"bwd\" else 3.5 * f)\n",
    "\n",
    "def efficiency(flop, time):\n",
    "    return (flop / time / 10**12) if not math.isnan(time) else 0.0\n",
    "\n",
    "\n",
    "def attention_pytorch(qkv, dropout_p=0.0, causal=True):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        qkv: (batch_size, seqlen, 3, nheads, head_dim)\n",
    "        dropout_p: float\n",
    "    Output:\n",
    "        output: (batch_size, seqlen, nheads, head_dim)\n",
    "    \"\"\"\n",
    "    batch_size, seqlen, _, nheads, d = qkv.shape\n",
    "    q, k, v = qkv.unbind(dim=2)\n",
    "    q = rearrange(q, 'b t h d -> (b h) t d')\n",
    "    k = rearrange(k, 'b s h d -> (b h) d s')\n",
    "    softmax_scale = 1.0 / math.sqrt(d)\n",
    "    # Preallocate attn_weights for `baddbmm`\n",
    "    scores = torch.empty(batch_size * nheads, seqlen, seqlen, dtype=qkv.dtype, device=qkv.device)\n",
    "    scores = rearrange(torch.baddbmm(scores, q, k, beta=0, alpha=softmax_scale),\n",
    "                       '(b h) t s -> b h t s', h=nheads)\n",
    "    if causal:\n",
    "        # \"triu_tril_cuda_template\" not implemented for 'BFloat16'\n",
    "        # So we have to construct the mask in float\n",
    "        causal_mask = torch.triu(torch.full((seqlen, seqlen), -10000.0, device=scores.device), 1)\n",
    "        # TD [2022-09-30]: Adding is faster than masked_fill_ (idk why, just better kernel I guess)\n",
    "        scores = scores + causal_mask.to(dtype=scores.dtype)\n",
    "    attention = torch.softmax(scores, dim=-1)\n",
    "    attention_drop = F.dropout(attention, dropout_p)\n",
    "    output = torch.einsum('bhts,bshd->bthd', attention_drop , v)\n",
    "    return output.to(dtype=qkv.dtype)\n",
    "\n",
    "\n",
    "def time_fwd_bwd(func, *args, **kwargs):\n",
    "    time_f, time_b = benchmark_fwd_bwd(func, *args, **kwargs)\n",
    "    return time_f[1].mean, time_b[1].mean\n",
    "\n",
    "\n",
    "repeats = 30\n",
    "device = 'cuda'\n",
    "dtype = torch.float16\n",
    "\n",
    "bs_seqlen_vals = [(32, 512), (16, 1024), (8, 2048), (4, 4096), (2, 8192), (1, 16384)]\n",
    "causal_vals = [False, True]\n",
    "headdim_vals = [64, 128]\n",
    "dim = 2048\n",
    "dropout_p = 0.0\n",
    "\n",
    "methods = ([\"Flash2\", \"Pytorch\"]\n",
    "           + ([\"Triton\"] if attention_triton is not None else [])\n",
    "           + ([\"xformers.c\"] if xops is not None else [])\n",
    "           + ([\"xformers.f\"] if xops is not None else []))\n",
    "\n",
    "time_f = {}\n",
    "time_b = {}\n",
    "time_f_b = {}\n",
    "speed_f = {}\n",
    "speed_b = {}\n",
    "speed_f_b = {}\n",
    "for causal in causal_vals:\n",
    "    for headdim in headdim_vals:\n",
    "        for batch_size, seqlen in bs_seqlen_vals:\n",
    "            config = (causal, headdim, batch_size, seqlen)\n",
    "            nheads = dim // headdim\n",
    "            qkv = torch.randn(batch_size, seqlen, 3, nheads, headdim, device=device, dtype=dtype,\n",
    "                              requires_grad=True)\n",
    "            f, b = time_fwd_bwd(\n",
    "                flash_attn_qkvpacked_func, qkv, dropout_p, causal=causal, repeats=repeats, verbose=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the newest triton version with\n",
    "# pip install \"git+https://github.com/openai/triton.git#egg=triton&subdirectory=python\"\n",
    "import pickle\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from flash_attn.utils.benchmark import benchmark_all, benchmark_forward, benchmark_backward\n",
    "from flash_attn.utils.benchmark import benchmark_fwd_bwd, benchmark_combined\n",
    "\n",
    "from flash_attn import flash_attn_qkvpacked_func\n",
    "\n",
    "try:\n",
    "    from triton.ops.flash_attention import attention as attention_triton\n",
    "except ImportError:\n",
    "    attention_triton = None\n",
    "\n",
    "try:\n",
    "    import xformers.ops as xops\n",
    "except ImportError:\n",
    "    xops = None\n",
    "\n",
    "\n",
    "def flops(batch, seqlen, headdim, nheads, causal, mode=\"fwd\"):\n",
    "    assert mode in [\"fwd\", \"bwd\", \"fwd_bwd\"]\n",
    "    f = 4 * batch * seqlen**2 * nheads * headdim // (2 if causal else 1)\n",
    "    return f if mode == \"fwd\" else (2.5 * f if mode == \"bwd\" else 3.5 * f)\n",
    "\n",
    "def efficiency(flop, time):\n",
    "    return (flop / time / 10**12) if not math.isnan(time) else 0.0\n",
    "\n",
    "\n",
    "def attention_pytorch(qkv, dropout_p=0.0, causal=True):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        qkv: (batch_size, seqlen, 3, nheads, head_dim)\n",
    "        dropout_p: float\n",
    "    Output:\n",
    "        output: (batch_size, seqlen, nheads, head_dim)\n",
    "    \"\"\"\n",
    "    batch_size, seqlen, _, nheads, d = qkv.shape\n",
    "    q, k, v = qkv.unbind(dim=2)\n",
    "    q = rearrange(q, 'b t h d -> (b h) t d')\n",
    "    k = rearrange(k, 'b s h d -> (b h) d s')\n",
    "    softmax_scale = 1.0 / math.sqrt(d)\n",
    "    # Preallocate attn_weights for `baddbmm`\n",
    "    scores = torch.empty(batch_size * nheads, seqlen, seqlen, dtype=qkv.dtype, device=qkv.device)\n",
    "    scores = rearrange(torch.baddbmm(scores, q, k, beta=0, alpha=softmax_scale),\n",
    "                       '(b h) t s -> b h t s', h=nheads)\n",
    "    if causal:\n",
    "        # \"triu_tril_cuda_template\" not implemented for 'BFloat16'\n",
    "        # So we have to construct the mask in float\n",
    "        causal_mask = torch.triu(torch.full((seqlen, seqlen), -10000.0, device=scores.device), 1)\n",
    "        # TD [2022-09-30]: Adding is faster than masked_fill_ (idk why, just better kernel I guess)\n",
    "        scores = scores + causal_mask.to(dtype=scores.dtype)\n",
    "    attention = torch.softmax(scores, dim=-1)\n",
    "    attention_drop = F.dropout(attention, dropout_p)\n",
    "    output = torch.einsum('bhts,bshd->bthd', attention_drop , v)\n",
    "    return output.to(dtype=qkv.dtype)\n",
    "\n",
    "\n",
    "def time_fwd_bwd(func, *args, **kwargs):\n",
    "    time_f, time_b = benchmark_fwd_bwd(func, *args, **kwargs)\n",
    "    return time_f[1].mean, time_b[1].mean\n",
    "\n",
    "\n",
    "repeats = 30\n",
    "device = 'cuda'\n",
    "dtype = torch.float16\n",
    "\n",
    "bs_seqlen_vals = [(32, 512), (16, 1024), (8, 2048), (4, 4096), (2, 8192), (1, 16384)]\n",
    "causal_vals = [False, True]\n",
    "headdim_vals = [64, 128]\n",
    "dim = 2048\n",
    "dropout_p = 0.0\n",
    "\n",
    "methods = ([\"Flash2\", \"Pytorch\"]\n",
    "           + ([\"Triton\"] if attention_triton is not None else [])\n",
    "           + ([\"xformers.c\"] if xops is not None else [])\n",
    "           + ([\"xformers.f\"] if xops is not None else []))\n",
    "\n",
    "time_f = {}\n",
    "time_b = {}\n",
    "time_f_b = {}\n",
    "speed_f = {}\n",
    "speed_b = {}\n",
    "speed_f_b = {}\n",
    "for causal in causal_vals:\n",
    "    for headdim in headdim_vals:\n",
    "        for batch_size, seqlen in bs_seqlen_vals:\n",
    "            config = (causal, headdim, batch_size, seqlen)\n",
    "            nheads = dim // headdim\n",
    "            qkv = torch.randn(batch_size, seqlen, 3, nheads, headdim, device=device, dtype=dtype,\n",
    "                              requires_grad=True)\n",
    "            f, b = benchmark_fwd_bwd(\n",
    "                flash_attn_qkvpacked_func, qkv, dropout_p, causal=causal, repeats=repeats, verbose=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00659653153270483"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
