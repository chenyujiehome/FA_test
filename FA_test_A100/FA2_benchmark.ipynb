{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535cbea0-fc5c-4529-b715-d35fb3e0f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the newest triton version with\n",
    "# pip install \"git+https://github.com/openai/triton.git#egg=triton&subdirectory=python\"\n",
    "import pickle\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from flash_attn.utils.benchmark import benchmark_all, benchmark_forward, benchmark_backward\n",
    "from flash_attn.utils.benchmark import benchmark_fwd_bwd, benchmark_combined\n",
    "\n",
    "from flash_attn import flash_attn_qkvpacked_func\n",
    "\n",
    "try:\n",
    "    from triton.ops.flash_attention import attention as attention_triton\n",
    "except ImportError:\n",
    "    attention_triton = None\n",
    "\n",
    "try:\n",
    "    import xformers.ops as xops\n",
    "except ImportError:\n",
    "    xops = None\n",
    "\n",
    "\n",
    "def flops(batch, seqlen, headdim, nheads, causal, mode=\"fwd\"):\n",
    "    assert mode in [\"fwd\", \"bwd\", \"fwd_bwd\"]\n",
    "    f = 4 * batch * seqlen**2 * nheads * headdim // (2 if causal else 1)\n",
    "    return f if mode == \"fwd\" else (2.5 * f if mode == \"bwd\" else 3.5 * f)\n",
    "\n",
    "def efficiency(flop, time):\n",
    "    return (flop / time / 10**12) if not math.isnan(time) else 0.0\n",
    "\n",
    "\n",
    "def attention_pytorch(qkv, dropout_p=0.0, causal=True):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        qkv: (batch_size, seqlen, 3, nheads, head_dim)\n",
    "        dropout_p: float\n",
    "    Output:\n",
    "        output: (batch_size, seqlen, nheads, head_dim)\n",
    "    \"\"\"\n",
    "    batch_size, seqlen, _, nheads, d = qkv.shape\n",
    "    q, k, v = qkv.unbind(dim=2)\n",
    "    q = rearrange(q, 'b t h d -> (b h) t d')\n",
    "    k = rearrange(k, 'b s h d -> (b h) d s')\n",
    "    softmax_scale = 1.0 / math.sqrt(d)\n",
    "    # Preallocate attn_weights for `baddbmm`\n",
    "    scores = torch.empty(batch_size * nheads, seqlen, seqlen, dtype=qkv.dtype, device=qkv.device)\n",
    "    scores = rearrange(torch.baddbmm(scores, q, k, beta=0, alpha=softmax_scale),\n",
    "                       '(b h) t s -> b h t s', h=nheads)\n",
    "    if causal:\n",
    "        # \"triu_tril_cuda_template\" not implemented for 'BFloat16'\n",
    "        # So we have to construct the mask in float\n",
    "        causal_mask = torch.triu(torch.full((seqlen, seqlen), -10000.0, device=scores.device), 1)\n",
    "        # TD [2022-09-30]: Adding is faster than masked_fill_ (idk why, just better kernel I guess)\n",
    "        scores = scores + causal_mask.to(dtype=scores.dtype)\n",
    "    attention = torch.softmax(scores, dim=-1)\n",
    "    attention_drop = F.dropout(attention, dropout_p)\n",
    "    output = torch.einsum('bhts,bshd->bthd', attention_drop , v)\n",
    "    return output.to(dtype=qkv.dtype)\n",
    "\n",
    "\n",
    "def time_fwd_bwd(func, *args, **kwargs):\n",
    "    time_f, time_b = benchmark_fwd_bwd(func, *args, **kwargs)\n",
    "    return time_f[1].mean, time_b[1].mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6cadaa5-9c79-42cd-8957-f9ebee2c43f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 30\n",
    "device = 'cuda'\n",
    "dtype = torch.float16\n",
    "\n",
    "bs_seqlen_vals = [(32, 512), (16, 1024), (8, 2048), (4, 4096), (2, 8192), (1, 16384)]\n",
    "causal_vals = [False, True]\n",
    "headdim_vals = [64, 128]\n",
    "dim = 2048\n",
    "dropout_p = 0.0\n",
    "\n",
    "methods = ([\"Flash2\", \"Pytorch\"]\n",
    "           + ([\"Triton\"] if attention_triton is not None else [])\n",
    "           + ([\"xformers.c\"] if xops is not None else [])\n",
    "           + ([\"xformers.f\"] if xops is not None else []))\n",
    "\n",
    "time_f = {}\n",
    "time_b = {}\n",
    "time_f_b = {}\n",
    "speed_f = {}\n",
    "speed_b = {}\n",
    "speed_f_b = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cf71521-7fa8-487a-912c-024122368baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal=False\n",
    "headdim=64\n",
    "batch_size,seqlen=(32,512)\n",
    "config = (causal, headdim, batch_size, seqlen)\n",
    "nheads = dim // headdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7aae5f9-a42a-4ef7-8d9f-0f02ba92ac6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512, 3, 32, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = torch.randn(batch_size, seqlen, 3, nheads, headdim, device=device, dtype=dtype,\n",
    "              requires_grad=True)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a79a8f-88d5-4f11-b77b-89c707cd25d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1df09616-ef98-4594-9420-a4f98e2bf9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_f, time_b = time_fwd_bwd(\n",
    "flash_attn_qkvpacked_func, qkv, dropout_p, causal=causal, repeats=repeats, verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54bbeb13-d798-4e70-a70e-0e787bdd0a96",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtime_b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "time_b.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809de074-41a4-40a2-80a9-82b01a48f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_f[config, \"Flash2\"] = f\n",
    "time_b[config, \"Flash2\"] = b\n",
    "\n",
    "try:\n",
    "qkv = qkv.detach().requires_grad_(True)\n",
    "f, b = time_fwd_bwd(\n",
    "    attention_pytorch, qkv, dropout_p, causal=causal, repeats=repeats, verbose=False\n",
    ")\n",
    "except:  # Skip if OOM\n",
    "f, b = float('nan'), float('nan')\n",
    "time_f[config, \"Pytorch\"] = f\n",
    "time_b[config, \"Pytorch\"] = b\n",
    "\n",
    "if attention_triton is not None:\n",
    "q, k, v = [torch.randn(batch_size, nheads, seqlen, headdim, device=device, dtype=dtype,\n",
    "                    requires_grad=True) for _ in range(3)]\n",
    "# Try both values of sequence_parallel and pick the faster one\n",
    "try:\n",
    "    f, b = time_fwd_bwd(\n",
    "        attention_triton, q, k, v, causal, headdim**(-0.5),\n",
    "        False, repeats=repeats, verbose=False\n",
    "    )\n",
    "except:\n",
    "    f, b = float('nan'), float('inf')\n",
    "try:\n",
    "    _, b0 = time_fwd_bwd(\n",
    "        attention_triton, q, k, v, causal, headdim**(-0.5),\n",
    "        True, repeats=repeats, verbose=False\n",
    "    )\n",
    "except:\n",
    "    b0 = float('inf')\n",
    "time_f[config, \"Triton\"] = f\n",
    "time_b[config, \"Triton\"] = min(b, b0) if min(b, b0) < float('inf') else float('nan')\n",
    "\n",
    "if xops is not None:\n",
    "q, k, v = [torch.randn(batch_size, seqlen, nheads, headdim, device=device, dtype=dtype,\n",
    "                    requires_grad=True) for _ in range(3)]\n",
    "f, b = time_fwd_bwd(\n",
    "    xops.memory_efficient_attention, q, k, v,\n",
    "    attn_bias=xops.LowerTriangularMask() if causal else None,\n",
    "    op=(xops.fmha.cutlass.FwOp, xops.fmha.cutlass.BwOp)\n",
    ")\n",
    "time_f[config, \"xformers.c\"] = f\n",
    "time_b[config, \"xformers.c\"] = b\n",
    "\n",
    "if xops is not None:\n",
    "q, k, v = [torch.randn(batch_size, seqlen, nheads, headdim, device=device, dtype=dtype,\n",
    "                    requires_grad=True) for _ in range(3)]\n",
    "f, b = time_fwd_bwd(\n",
    "    xops.memory_efficient_attention, q, k, v,\n",
    "    attn_bias=xops.LowerTriangularMask() if causal else None,\n",
    "    op=(xops.fmha.flash.FwOp, xops.fmha.flash.BwOp)\n",
    ")\n",
    "time_f[config, \"xformers.f\"] = f\n",
    "time_b[config, \"xformers.f\"] = b\n",
    "\n",
    "print(f\"### causal={causal}, headdim={headdim}, batch_size={batch_size}, seqlen={seqlen} ###\")\n",
    "for method in methods:\n",
    "time_f_b[config, method] = time_f[config, method] + time_b[config, method]\n",
    "speed_f[config, method] = efficiency(\n",
    "    flops(batch_size, seqlen, headdim, nheads, causal, mode=\"fwd\"),\n",
    "    time_f[config, method]\n",
    ")\n",
    "speed_b[config, method] = efficiency(\n",
    "    flops(batch_size, seqlen, headdim, nheads, causal, mode=\"bwd\"),\n",
    "    time_b[config, method]\n",
    ")\n",
    "speed_f_b[config, method] = efficiency(\n",
    "    flops(batch_size, seqlen, headdim, nheads, causal, mode=\"fwd_bwd\"),\n",
    "    time_f_b[config, method]\n",
    ")\n",
    "print(\n",
    "    f\"{method} fwd: {speed_f[config, method]:.2f} TFLOPs/s, \"\n",
    "    f\"bwd: {speed_b[config, method]:.2f} TFLOPs/s, \"\n",
    "    f\"fwd + bwd: {speed_f_b[config, method]:.2f} TFLOPs/s\"\n",
    ")\n",
    "\n",
    "\n",
    "# with open('flash2_attn_time.plk', 'wb') as fp:\n",
    "#     pickle.dump((speed_f, speed_b, speed_f_b), fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "547d0aff-ed9f-4956-b021-71febc7bed88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### causal=False, headdim=64, batch_size=32, seqlen=512 ###\n",
      "Flash2 fwd: 99.82 TFLOPs/s, bwd: 73.42 TFLOPs/s, fwd + bwd: 79.42 TFLOPs/s\n",
      "Pytorch fwd: 19.99 TFLOPs/s, bwd: 30.58 TFLOPs/s, fwd + bwd: 26.56 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=False, headdim=64, batch_size=16, seqlen=1024 ###\n",
      "Flash2 fwd: 174.61 TFLOPs/s, bwd: 123.78 TFLOPs/s, fwd + bwd: 135.01 TFLOPs/s\n",
      "Pytorch fwd: 27.91 TFLOPs/s, bwd: 35.94 TFLOPs/s, fwd + bwd: 33.21 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=False, headdim=64, batch_size=8, seqlen=2048 ###\n",
      "Flash2 fwd: 174.09 TFLOPs/s, bwd: 136.91 TFLOPs/s, fwd + bwd: 145.80 TFLOPs/s\n",
      "Pytorch fwd: 29.72 TFLOPs/s, bwd: 38.52 TFLOPs/s, fwd + bwd: 35.51 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=False, headdim=64, batch_size=4, seqlen=4096 ###\n",
      "Flash2 fwd: 173.78 TFLOPs/s, bwd: 147.43 TFLOPs/s, fwd + bwd: 154.10 TFLOPs/s\n",
      "Pytorch fwd: 31.49 TFLOPs/s, bwd: 40.00 TFLOPs/s, fwd + bwd: 37.13 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=False, headdim=64, batch_size=2, seqlen=8192 ###\n",
      "Flash2 fwd: 169.27 TFLOPs/s, bwd: 153.06 TFLOPs/s, fwd + bwd: 157.37 TFLOPs/s\n",
      "Pytorch fwd: 32.24 TFLOPs/s, bwd: 41.23 TFLOPs/s, fwd + bwd: 38.19 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=False, headdim=64, batch_size=1, seqlen=16384 ###\n",
      "Flash2 fwd: 170.84 TFLOPs/s, bwd: 155.37 TFLOPs/s, fwd + bwd: 159.49 TFLOPs/s\n",
      "Pytorch fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=False, headdim=128, batch_size=32, seqlen=512 ###\n",
      "Flash2 fwd: 188.78 TFLOPs/s, bwd: 123.62 TFLOPs/s, fwd + bwd: 137.14 TFLOPs/s\n",
      "Pytorch fwd: 35.72 TFLOPs/s, bwd: 49.47 TFLOPs/s, fwd + bwd: 44.56 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=False, headdim=128, batch_size=16, seqlen=1024 ###\n",
      "Flash2 fwd: 196.03 TFLOPs/s, bwd: 136.93 TFLOPs/s, fwd + bwd: 149.83 TFLOPs/s\n",
      "Pytorch fwd: 46.47 TFLOPs/s, bwd: 61.26 TFLOPs/s, fwd + bwd: 56.15 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=False, headdim=128, batch_size=8, seqlen=2048 ###\n",
      "Flash2 fwd: 194.97 TFLOPs/s, bwd: 153.56 TFLOPs/s, fwd + bwd: 163.48 TFLOPs/s\n",
      "Pytorch fwd: 49.40 TFLOPs/s, bwd: 69.75 TFLOPs/s, fwd + bwd: 62.40 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=False, headdim=128, batch_size=4, seqlen=4096 ###\n",
      "Flash2 fwd: 190.54 TFLOPs/s, bwd: 164.39 TFLOPs/s, fwd + bwd: 171.10 TFLOPs/s\n",
      "Pytorch fwd: 54.37 TFLOPs/s, bwd: 71.78 TFLOPs/s, fwd + bwd: 65.76 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=False, headdim=128, batch_size=2, seqlen=8192 ###\n",
      "Flash2 fwd: 192.41 TFLOPs/s, bwd: 169.51 TFLOPs/s, fwd + bwd: 175.47 TFLOPs/s\n",
      "Pytorch fwd: 57.89 TFLOPs/s, bwd: 78.32 TFLOPs/s, fwd + bwd: 71.15 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=False, headdim=128, batch_size=1, seqlen=16384 ###\n",
      "Flash2 fwd: 192.57 TFLOPs/s, bwd: 171.79 TFLOPs/s, fwd + bwd: 177.26 TFLOPs/s\n",
      "Pytorch fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=True, headdim=64, batch_size=32, seqlen=512 ###\n",
      "Flash2 fwd: 109.73 TFLOPs/s, bwd: 73.34 TFLOPs/s, fwd + bwd: 81.02 TFLOPs/s\n",
      "Pytorch fwd: 8.63 TFLOPs/s, bwd: 16.08 TFLOPs/s, fwd + bwd: 12.90 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=True, headdim=64, batch_size=16, seqlen=1024 ###\n",
      "Flash2 fwd: 138.66 TFLOPs/s, bwd: 95.79 TFLOPs/s, fwd + bwd: 105.08 TFLOPs/s\n",
      "Pytorch fwd: 9.19 TFLOPs/s, bwd: 17.98 TFLOPs/s, fwd + bwd: 14.12 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=True, headdim=64, batch_size=8, seqlen=2048 ###\n",
      "Flash2 fwd: 157.99 TFLOPs/s, bwd: 114.01 TFLOPs/s, fwd + bwd: 123.86 TFLOPs/s\n",
      "Pytorch fwd: 9.19 TFLOPs/s, bwd: 19.31 TFLOPs/s, fwd + bwd: 14.68 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=True, headdim=64, batch_size=4, seqlen=4096 ###\n",
      "Flash2 fwd: 160.12 TFLOPs/s, bwd: 129.23 TFLOPs/s, fwd + bwd: 136.77 TFLOPs/s\n",
      "Pytorch fwd: 9.18 TFLOPs/s, bwd: 20.05 TFLOPs/s, fwd + bwd: 14.98 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=True, headdim=64, batch_size=2, seqlen=8192 ###\n",
      "Flash2 fwd: 169.21 TFLOPs/s, bwd: 141.39 TFLOPs/s, fwd + bwd: 148.36 TFLOPs/s\n",
      "Pytorch fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=True, headdim=64, batch_size=1, seqlen=16384 ###\n",
      "Flash2 fwd: 163.41 TFLOPs/s, bwd: 149.22 TFLOPs/s, fwd + bwd: 153.02 TFLOPs/s\n",
      "Pytorch fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=True, headdim=128, batch_size=32, seqlen=512 ###\n",
      "Flash2 fwd: 121.76 TFLOPs/s, bwd: 82.59 TFLOPs/s, fwd + bwd: 90.95 TFLOPs/s\n",
      "Pytorch fwd: 13.43 TFLOPs/s, bwd: 24.78 TFLOPs/s, fwd + bwd: 19.96 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=True, headdim=128, batch_size=16, seqlen=1024 ###\n",
      "Flash2 fwd: 148.49 TFLOPs/s, bwd: 105.47 TFLOPs/s, fwd + bwd: 114.99 TFLOPs/s\n",
      "Pytorch fwd: 15.68 TFLOPs/s, bwd: 30.90 TFLOPs/s, fwd + bwd: 24.19 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=True, headdim=128, batch_size=8, seqlen=2048 ###\n",
      "Flash2 fwd: 167.36 TFLOPs/s, bwd: 128.33 TFLOPs/s, fwd + bwd: 137.49 TFLOPs/s\n",
      "Pytorch fwd: 16.33 TFLOPs/s, bwd: 35.17 TFLOPs/s, fwd + bwd: 26.45 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=True, headdim=128, batch_size=4, seqlen=4096 ###\n",
      "Flash2 fwd: 172.53 TFLOPs/s, bwd: 144.35 TFLOPs/s, fwd + bwd: 151.42 TFLOPs/s\n",
      "Pytorch fwd: 16.71 TFLOPs/s, bwd: 36.24 TFLOPs/s, fwd + bwd: 27.17 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=True, headdim=128, batch_size=2, seqlen=8192 ###\n",
      "Flash2 fwd: 168.87 TFLOPs/s, bwd: 157.02 TFLOPs/s, fwd + bwd: 160.23 TFLOPs/s\n",
      "Pytorch fwd: 17.13 TFLOPs/s, bwd: 39.42 TFLOPs/s, fwd + bwd: 28.74 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "### causal=True, headdim=128, batch_size=1, seqlen=16384 ###\n",
      "Flash2 fwd: 172.03 TFLOPs/s, bwd: 165.55 TFLOPs/s, fwd + bwd: 167.35 TFLOPs/s\n",
      "Pytorch fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n",
      "Triton fwd: 0.00 TFLOPs/s, bwd: 0.00 TFLOPs/s, fwd + bwd: 0.00 TFLOPs/s\n"
     ]
    }
   ],
   "source": [
    "# Install the newest triton version with\n",
    "# pip install \"git+https://github.com/openai/triton.git#egg=triton&subdirectory=python\"\n",
    "import pickle\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from flash_attn.utils.benchmark import benchmark_all, benchmark_forward, benchmark_backward\n",
    "from flash_attn.utils.benchmark import benchmark_fwd_bwd, benchmark_combined\n",
    "\n",
    "from flash_attn import flash_attn_qkvpacked_func\n",
    "\n",
    "try:\n",
    "    from triton.ops.flash_attention import attention as attention_triton\n",
    "except ImportError:\n",
    "    attention_triton = None\n",
    "\n",
    "try:\n",
    "    import xformers.ops as xops\n",
    "except ImportError:\n",
    "    xops = None\n",
    "\n",
    "\n",
    "def flops(batch, seqlen, headdim, nheads, causal, mode=\"fwd\"):\n",
    "    assert mode in [\"fwd\", \"bwd\", \"fwd_bwd\"]\n",
    "    f = 4 * batch * seqlen**2 * nheads * headdim // (2 if causal else 1)\n",
    "    return f if mode == \"fwd\" else (2.5 * f if mode == \"bwd\" else 3.5 * f)\n",
    "\n",
    "def efficiency(flop, time):\n",
    "    return (flop / time / 10**12) if not math.isnan(time) else 0.0\n",
    "\n",
    "\n",
    "def attention_pytorch(qkv, dropout_p=0.0, causal=True):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        qkv: (batch_size, seqlen, 3, nheads, head_dim)\n",
    "        dropout_p: float\n",
    "    Output:\n",
    "        output: (batch_size, seqlen, nheads, head_dim)\n",
    "    \"\"\"\n",
    "    batch_size, seqlen, _, nheads, d = qkv.shape\n",
    "    q, k, v = qkv.unbind(dim=2)\n",
    "    q = rearrange(q, 'b t h d -> (b h) t d')\n",
    "    k = rearrange(k, 'b s h d -> (b h) d s')\n",
    "    softmax_scale = 1.0 / math.sqrt(d)\n",
    "    # Preallocate attn_weights for `baddbmm`\n",
    "    scores = torch.empty(batch_size * nheads, seqlen, seqlen, dtype=qkv.dtype, device=qkv.device)\n",
    "    scores = rearrange(torch.baddbmm(scores, q, k, beta=0, alpha=softmax_scale),\n",
    "                       '(b h) t s -> b h t s', h=nheads)\n",
    "    if causal:\n",
    "        # \"triu_tril_cuda_template\" not implemented for 'BFloat16'\n",
    "        # So we have to construct the mask in float\n",
    "        causal_mask = torch.triu(torch.full((seqlen, seqlen), -10000.0, device=scores.device), 1)\n",
    "        # TD [2022-09-30]: Adding is faster than masked_fill_ (idk why, just better kernel I guess)\n",
    "        scores = scores + causal_mask.to(dtype=scores.dtype)\n",
    "    attention = torch.softmax(scores, dim=-1)\n",
    "    attention_drop = F.dropout(attention, dropout_p)\n",
    "    output = torch.einsum('bhts,bshd->bthd', attention_drop , v)\n",
    "    return output.to(dtype=qkv.dtype)\n",
    "\n",
    "\n",
    "def time_fwd_bwd(func, *args, **kwargs):\n",
    "    time_f, time_b = benchmark_fwd_bwd(func, *args, **kwargs)\n",
    "    return time_f[1].mean, time_b[1].mean\n",
    "\n",
    "\n",
    "repeats = 30\n",
    "device = 'cuda'\n",
    "dtype = torch.float16\n",
    "\n",
    "bs_seqlen_vals = [(32, 512), (16, 1024), (8, 2048), (4, 4096), (2, 8192), (1, 16384)]\n",
    "causal_vals = [False, True]\n",
    "headdim_vals = [64, 128]\n",
    "dim = 2048\n",
    "dropout_p = 0.0\n",
    "\n",
    "methods = ([\"Flash2\", \"Pytorch\"]\n",
    "           + ([\"Triton\"] if attention_triton is not None else [])\n",
    "           + ([\"xformers.c\"] if xops is not None else [])\n",
    "           + ([\"xformers.f\"] if xops is not None else []))\n",
    "\n",
    "time_f = {}\n",
    "time_b = {}\n",
    "time_f_b = {}\n",
    "speed_f = {}\n",
    "speed_b = {}\n",
    "speed_f_b = {}\n",
    "for causal in causal_vals:\n",
    "    for headdim in headdim_vals:\n",
    "        for batch_size, seqlen in bs_seqlen_vals:\n",
    "            config = (causal, headdim, batch_size, seqlen)\n",
    "            nheads = dim // headdim\n",
    "            qkv = torch.randn(batch_size, seqlen, 3, nheads, headdim, device=device, dtype=dtype,\n",
    "                              requires_grad=True)\n",
    "            f, b = time_fwd_bwd(\n",
    "                flash_attn_qkvpacked_func, qkv, dropout_p, causal=causal, repeats=repeats, verbose=False\n",
    "            )\n",
    "            time_f[config, \"Flash2\"] = f\n",
    "            time_b[config, \"Flash2\"] = b\n",
    "\n",
    "            try:\n",
    "                qkv = qkv.detach().requires_grad_(True)\n",
    "                f, b = time_fwd_bwd(\n",
    "                    attention_pytorch, qkv, dropout_p, causal=causal, repeats=repeats, verbose=False\n",
    "                )\n",
    "            except:  # Skip if OOM\n",
    "                f, b = float('nan'), float('nan')\n",
    "            time_f[config, \"Pytorch\"] = f\n",
    "            time_b[config, \"Pytorch\"] = b\n",
    "\n",
    "            if attention_triton is not None:\n",
    "                q, k, v = [torch.randn(batch_size, nheads, seqlen, headdim, device=device, dtype=dtype,\n",
    "                                    requires_grad=True) for _ in range(3)]\n",
    "                # Try both values of sequence_parallel and pick the faster one\n",
    "                try:\n",
    "                    f, b = time_fwd_bwd(\n",
    "                        attention_triton, q, k, v, causal, headdim**(-0.5),\n",
    "                        False, repeats=repeats, verbose=False\n",
    "                    )\n",
    "                except:\n",
    "                    f, b = float('nan'), float('inf')\n",
    "                try:\n",
    "                    _, b0 = time_fwd_bwd(\n",
    "                        attention_triton, q, k, v, causal, headdim**(-0.5),\n",
    "                        True, repeats=repeats, verbose=False\n",
    "                    )\n",
    "                except:\n",
    "                    b0 = float('inf')\n",
    "                time_f[config, \"Triton\"] = f\n",
    "                time_b[config, \"Triton\"] = min(b, b0) if min(b, b0) < float('inf') else float('nan')\n",
    "\n",
    "            if xops is not None:\n",
    "                q, k, v = [torch.randn(batch_size, seqlen, nheads, headdim, device=device, dtype=dtype,\n",
    "                                    requires_grad=True) for _ in range(3)]\n",
    "                f, b = time_fwd_bwd(\n",
    "                    xops.memory_efficient_attention, q, k, v,\n",
    "                    attn_bias=xops.LowerTriangularMask() if causal else None,\n",
    "                    op=(xops.fmha.cutlass.FwOp, xops.fmha.cutlass.BwOp)\n",
    "                )\n",
    "                time_f[config, \"xformers.c\"] = f\n",
    "                time_b[config, \"xformers.c\"] = b\n",
    "\n",
    "            if xops is not None:\n",
    "                q, k, v = [torch.randn(batch_size, seqlen, nheads, headdim, device=device, dtype=dtype,\n",
    "                                    requires_grad=True) for _ in range(3)]\n",
    "                f, b = time_fwd_bwd(\n",
    "                    xops.memory_efficient_attention, q, k, v,\n",
    "                    attn_bias=xops.LowerTriangularMask() if causal else None,\n",
    "                    op=(xops.fmha.flash.FwOp, xops.fmha.flash.BwOp)\n",
    "                )\n",
    "                time_f[config, \"xformers.f\"] = f\n",
    "                time_b[config, \"xformers.f\"] = b\n",
    "\n",
    "            print(f\"### causal={causal}, headdim={headdim}, batch_size={batch_size}, seqlen={seqlen} ###\")\n",
    "            for method in methods:\n",
    "                time_f_b[config, method] = time_f[config, method] + time_b[config, method]\n",
    "                speed_f[config, method] = efficiency(\n",
    "                    flops(batch_size, seqlen, headdim, nheads, causal, mode=\"fwd\"),\n",
    "                    time_f[config, method]\n",
    "                )\n",
    "                speed_b[config, method] = efficiency(\n",
    "                    flops(batch_size, seqlen, headdim, nheads, causal, mode=\"bwd\"),\n",
    "                    time_b[config, method]\n",
    "                )\n",
    "                speed_f_b[config, method] = efficiency(\n",
    "                    flops(batch_size, seqlen, headdim, nheads, causal, mode=\"fwd_bwd\"),\n",
    "                    time_f_b[config, method]\n",
    "                )\n",
    "                print(\n",
    "                    f\"{method} fwd: {speed_f[config, method]:.2f} TFLOPs/s, \"\n",
    "                    f\"bwd: {speed_b[config, method]:.2f} TFLOPs/s, \"\n",
    "                    f\"fwd + bwd: {speed_f_b[config, method]:.2f} TFLOPs/s\"\n",
    "                )\n",
    "\n",
    "\n",
    "# with open('flash2_attn_time.plk', 'wb') as fp:\n",
    "#     pickle.dump((speed_f, speed_b, speed_f_b), fp, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac409d22-c797-4052-906c-046e265def7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
